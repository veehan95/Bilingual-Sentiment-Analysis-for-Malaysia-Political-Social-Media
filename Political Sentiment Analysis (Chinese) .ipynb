{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political Sentiment Analysis using Tensorflow (English)\n",
    "---\n",
    "By Tan Vee Han\n",
    "\n",
    "Supervisor Dr. Victor Tan Hock Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "%matplotlib inline\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from tflearn.data_utils import VocabularyProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the file locaton according to the location you stored the dataset\n",
    "file = r'ch_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading raw dataset\n",
    "raw_data = pd.read_csv(file, encoding = \"utf-8-sig\")\n",
    "\n",
    "# Cloning the dataset\n",
    "data = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "semantic_value\n",
       "negative    466\n",
       "neutral     390\n",
       "positive    331\n",
       "Name: _id, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('semantic_value')['_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of alphabertic words in a dataset\n",
    "data['word_count'] = data.value.apply(lambda x: len(x))\n",
    "filter_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering labeled data\n",
    "pos_val = filter_data[filter_data.semantic_value == \"positive\"]\n",
    "neg_val = filter_data[filter_data.semantic_value == \"negative\"]\n",
    "neu_val = filter_data[filter_data.semantic_value == \"neutral\"]\n",
    "\n",
    "# Spliting into train and test dataset\n",
    "train_set = pd.concat([pos_val[:333], neg_val[:333], neu_val[:333]])\n",
    "test_set = pd.concat([pos_val[333:], neg_val[333:], neu_val[333:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid dataset: 157709\n",
      "Total number of train dataset: 997\n",
      "Total number of test dataset: 190\n"
     ]
    }
   ],
   "source": [
    "# Print dataset Info\n",
    "print(\"Total number of valid dataset:\", filter_data.loc[filter_data.semantic_value != \"unassigned\",'_id'].count())\n",
    "print(\"Total number of train dataset:\", train_set._id.count())\n",
    "print(\"Total number of test dataset:\", test_set._id.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b6b4734b38>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word count across the dataset\n",
    "filter_data.groupby('word_count')['_id'].count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    157709.000000\n",
       "mean         41.732336\n",
       "std          44.514157\n",
       "min           1.000000\n",
       "25%          16.000000\n",
       "50%          30.000000\n",
       "75%          54.000000\n",
       "max        1949.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data['word_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset size\n",
    "data_set_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into list\n",
    "train_pos = train_set.loc[train_set.semantic_value == \"positive\", \"value\"].tolist()\n",
    "train_neg = train_set.loc[train_set.semantic_value == \"negative\", \"value\"].tolist()\n",
    "test_pos = test_set.loc[test_set.semantic_value == \"positive\", \"value\"].tolist()\n",
    "test_neg = test_set.loc[test_set.semantic_value == \"negative\", \"value\"].tolist()\n",
    "    \n",
    "total_x = np.concatenate([train_pos, train_neg, test_pos, test_neg])\n",
    "train_y = np.concatenate([[[0, 1] for _ in range(data_set_size)], \n",
    "                          [[1, 0] for _ in range(data_set_size)]])\n",
    "test_y = np.concatenate([[[0, 1] for _ in range(data_set_size)], \n",
    "                         [[1, 0] for _ in range(data_set_size)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tflearn\\data_utils.py:201: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "# Convert reviews into padded word indexes\n",
    "num_words = 250 # Based on graph (want to capture most reviews without too much padding)\n",
    "vocab_processor = VocabularyProcessor(num_words).fit_transform(total_x)\n",
    "total_x_vector = np.array(list(vocab_processor))\n",
    "train_x_vector = total_x_vector[:500]\n",
    "test_x_vector = total_x_vector[500:]\n",
    "\n",
    "# Shuffle data\n",
    "shuffled1 = np.random.permutation(np.arange(len(train_x_vector)))\n",
    "train_x_vector_shuffled = train_x_vector[shuffled1]\n",
    "train_y_shuffled = train_y[shuffled1]\n",
    "shuffled2 = np.random.permutation(np.arange(len(test_x_vector)))\n",
    "test_x_vector_shuffled = test_x_vector[shuffled2]\n",
    "test_y_shuffled = test_y[shuffled2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-8126170c4433>:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\cloud\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Train epoch 0: loss = 107.22, train accuracy = 53.12%\n",
      "Train epoch 1: loss = 111.82, train accuracy = 53.12%\n",
      "Train epoch 2: loss = 103.24, train accuracy = 54.69%\n",
      "Train epoch 3: loss = 82.25, train accuracy = 57.81%\n",
      "Train epoch 4: loss = 75.37, train accuracy = 60.94%\n",
      "Train epoch 5: loss = 88.42, train accuracy = 50.00%\n",
      "Train epoch 6: loss = 67.02, train accuracy = 50.00%\n",
      "Train epoch 7: loss = 60.26, train accuracy = 54.69%\n",
      "Train epoch 8: loss = 75.77, train accuracy = 53.12%\n",
      "Train epoch 9: loss = 55.62, train accuracy = 51.56%\n",
      "Train epoch 10: loss = 43.67, train accuracy = 64.06%\n",
      "Test set accuracy: 86.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'02:51'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Variables\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(data_set_size * 2 / batch_size * epochs) # 3900\n",
    "reg_constant = 0.01\n",
    "num_classes = 2\n",
    "vocab_size = max([max(x) for x in total_x_vector]) + 1 # 101244\n",
    "embedding_size = 128 \n",
    "patch_size_1 = 3\n",
    "patch_size_2 = 4\n",
    "patch_size_3 = 5\n",
    "num_channels = 1\n",
    "conv_depth = 128\n",
    "conv_stride = [1, 1, 1, 1]\n",
    "pool_stride = [1, 1, 1, 1]\n",
    "padding = 'VALID'\n",
    "losses = []\n",
    "\n",
    "# Graph\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as session:\n",
    "        # Input data\n",
    "        data_x_tf = tf.placeholder(tf.int32, [batch_size, num_words])\n",
    "        data_y_tf = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "        dropout_tf = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Embeddeding\n",
    "        embedding_space = tf.Variable(tf.random_uniform([vocab_size, embedding_size])) \n",
    "        data_x_embedded = tf.nn.embedding_lookup(embedding_space, data_x_tf) \n",
    "        data_x_embedded_expanded = tf.expand_dims(data_x_embedded, -1) \n",
    "\n",
    "        # Fist convolution\n",
    "        conv_1_weights = tf.Variable(tf.truncated_normal([patch_size_1, \n",
    "                                                          embedding_size, \n",
    "                                                          num_channels, \n",
    "                                                          conv_depth]))\n",
    "        conv_1_biases = tf.Variable(tf.constant(0.1, shape=(conv_depth,)))\n",
    "        conv_1 = tf.nn.conv2d(data_x_embedded_expanded, conv_1_weights, conv_stride, padding)\n",
    "        conv_1_relu = tf.nn.relu(tf.nn.bias_add(conv_1, conv_1_biases))\n",
    "        pool_1 = tf.nn.max_pool(conv_1_relu, \n",
    "                                [1, num_words - patch_size_1 + 1, 1, 1], \n",
    "                                pool_stride, padding)\n",
    "\n",
    "        # Second convolution\n",
    "        conv_2_weights = tf.Variable(tf.truncated_normal([patch_size_2, \n",
    "                                                          embedding_size, \n",
    "                                                          num_channels, \n",
    "                                                          conv_depth]))\n",
    "        conv_2_biases = tf.Variable(tf.constant(0.1, shape=(conv_depth,)))\n",
    "        conv_2 = tf.nn.conv2d(data_x_embedded_expanded, conv_2_weights, conv_stride, padding)\n",
    "        conv_2_relu = tf.nn.relu(tf.nn.bias_add(conv_2, conv_2_biases))\n",
    "        pool_2 = tf.nn.max_pool(conv_2_relu, \n",
    "                                [1, num_words - patch_size_2 + 1, 1, 1], \n",
    "                                pool_stride, padding)\n",
    "\n",
    "        # Third convolution\n",
    "        conv_3_weights = tf.Variable(tf.truncated_normal([patch_size_3, \n",
    "                                                          embedding_size, \n",
    "                                                          num_channels, \n",
    "                                                          conv_depth]))\n",
    "        conv_3_biases = tf.Variable(tf.constant(0.1, shape=(conv_depth,)))\n",
    "        conv_3 = tf.nn.conv2d(data_x_embedded_expanded, conv_3_weights, conv_stride, padding)\n",
    "        conv_3_relu = tf.nn.relu(tf.nn.bias_add(conv_3, conv_3_biases))\n",
    "        pool_3 = tf.nn.max_pool(conv_3_relu, \n",
    "                                [1, num_words - patch_size_3 + 1, 1, 1], \n",
    "                                pool_stride, padding)\n",
    "\n",
    "        # Reshape\n",
    "        pool = tf.concat([pool_1, pool_2, pool_3], 3)\n",
    "        pool_shape = pool.get_shape().as_list()\n",
    "        reshaped_pool = tf.reshape(pool, [pool_shape[0],  pool_shape[3]])            \n",
    "        reshaped_pool_dropout = tf.nn.dropout(reshaped_pool, dropout_tf)           \n",
    "\n",
    "        # Output layer weights and biases\n",
    "        output_weights = tf.Variable(tf.truncated_normal([conv_depth * 3, num_classes]))\n",
    "        output_biases = tf.Variable(tf.constant(0.1, shape=(num_classes,)))\n",
    "        output = tf.nn.bias_add(tf.matmul(reshaped_pool_dropout, output_weights), \n",
    "                                output_biases)\n",
    "\n",
    "        # Loss, optimizer, and predictions\n",
    "        regularization = reg_constant * tf.nn.l2_loss(output_weights)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=data_y_tf) + \n",
    "                              regularization)\n",
    "        loss_summary = tf.summary.scalar('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        preds = tf.nn.softmax(output)\n",
    "\n",
    "        # Write accuracy to summary\n",
    "        sim = tf.equal(tf.argmax(preds, 1), tf.argmax(data_y_tf, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(sim, tf.float32)) * 100\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)           \n",
    "        merged_train = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        summary_writer_train = tf.summary.FileWriter('tensorboard/train', session.graph)\n",
    "        # Train model\n",
    "        tf.initialize_all_variables().run()\n",
    "        for i in range(num_steps):\n",
    "            batch_start = batch_size * i % (data_set_size * 2 - batch_size)\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_x = train_x_vector_shuffled[batch_start:batch_end,:]\n",
    "            batch_y = train_y_shuffled[batch_start:batch_end,:]            \n",
    "            feed_dict = {data_x_tf: batch_x, data_y_tf: batch_y, dropout_tf: 0.5}\n",
    "            if batch_x.shape == (64,250):\n",
    "                _, l, summary, accuracy_train = session.run([optimizer, \n",
    "                                                             loss, \n",
    "                                                             merged_train, \n",
    "                                                             accuracy], \n",
    "                                                            feed_dict) \n",
    "                summary_writer_train.add_summary(summary, i)            \n",
    "                if i % int(num_steps / epochs) == 0:\n",
    "                    epoch_num = int(i / int(num_steps / epochs))\n",
    "                    print('Train epoch %d: loss = %.2f, train accuracy = %.2f%%' \n",
    "                          % (epoch_num, l, accuracy_train))\n",
    "\n",
    "        # Test model\n",
    "        test_preds_total = []\n",
    "        for i in range(int(np.floor(data_set_size * 2 / batch_size))):\n",
    "            batch_start = batch_size * i\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_x = test_x_vector_shuffled[batch_start:batch_end,:]\n",
    "            batch_y = test_y_shuffled[batch_start:batch_end,:]\n",
    "            if batch_x.shape == (64,250):\n",
    "                feed_dict = {data_x_tf: batch_x, data_y_tf: batch_y, dropout_tf: 1.0}  \n",
    "                test_preds_total += (session.run([accuracy], feed_dict))\n",
    "        print('Test set accuracy: %.2f%%' % (sum(test_preds_total) / len(test_preds_total)))\n",
    "time.strftime(\"%M:%S\", time.gmtime(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
